{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf-8 -*-\n",
    "\"\"\"\n",
    "  This script provides an k-BERT exmaple for classification.\n",
    "\"\"\"\n",
    "import os\n",
    "import collections\n",
    "import codecs\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List\n",
    "from multiprocessing import Process, Pool\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (BertConfig, BertTokenizer, \n",
    "                          BertModel, BertPreTrainedModel,\n",
    "                          DistilBertConfig, DistilBertTokenizer,\n",
    "                          DistilBertModel, DistilBertPreTrainedModel)\n",
    "\n",
    "from uer.optimizers import BertAdam\n",
    "from brain import KnowledgeGraph\n",
    "from utils import set_seed, load_hyperparam, save_model\n",
    "from constants import * \n",
    "\n",
    "# task model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, args, model):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.labels_num = args.labels_num\n",
    "        self.bert = model\n",
    "        self.output_layer_2 = nn.Linear(args.hidden_size, args.labels_num)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.use_vm = False if args.no_vm else True\n",
    "        print(\"[BertClassifier] use visible_matrix: {}\".format(self.use_vm))\n",
    "\n",
    "    def forward(self, src, label, vm=None, seg=None, pos=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [batch_size x seq_length]\n",
    "            label: [batch_size]\n",
    "            mask: [batch_size x seq_length]\n",
    "        \"\"\"\n",
    "        # Encoder.\n",
    "        if not self.use_vm:\n",
    "            vm = None\n",
    "        output = self.bert(input_ids=src, attention_mask=vm, token_type_ids=seg, position_ids=pos, return_dict=True)\n",
    "        output = output['pooler_output']\n",
    "        logits = self.output_layer_2(output)\n",
    "        loss = self.criterion(self.softmax(logits.view(-1, self.labels_num)), label.view(-1))\n",
    "        return loss, logits\n",
    "\n",
    "def add_knowledge_worker(params):\n",
    "    p_id, sentences, columns, kg, bert_token, max_length = params # modified\n",
    "    sentences_num = len(sentences)\n",
    "    dataset = []\n",
    "    for line_id, line in enumerate(sentences):\n",
    "        if line_id % 10000 == 0:\n",
    "            print(\"Progress of process {}: {}/{}\".format(p_id, line_id, sentences_num))\n",
    "            sys.stdout.flush()\n",
    "        line = line.strip().split('\\t')\n",
    "        try:\n",
    "            label = int(line[columns[\"label\"]])\n",
    "            if len(line) == 2:\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]]\n",
    "   \n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=max_length) # modified\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0]\n",
    "                token_ids = bert_token.convert_tokens_to_ids(tokens)\n",
    "\n",
    "                seg = [0 for t in tokens]\n",
    "                \n",
    "                dataset.append((token_ids, label, seg, pos, vm))\n",
    "            \n",
    "            elif len(line)==3:                  \n",
    "                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n",
    "                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=max_length) # modified\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0]\n",
    "                token_ids = bert_token.convert_tokens_to_ids(tokens)\n",
    "\n",
    "                seg = []\n",
    "                seg_tag = 0\n",
    "                for t in tokens:\n",
    "                    seg.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "                \n",
    "                dataset.append((token_ids, label, seg, pos, vm))\n",
    "            \n",
    "            elif len(line) == 4:  # for dbqa\n",
    "                qid=int(line[columns[\"qid\"]])\n",
    "                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n",
    "                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=max_length) # modified\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0]\n",
    "                token_ids = bert_token.convert_tokens_to_ids(tokens)\n",
    "\n",
    "                seg = []\n",
    "                seg_tag = 0\n",
    "                for t in tokens:\n",
    "                    seg.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "                \n",
    "                dataset.append((token_ids, label, seg, pos, vm, qid))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        except:\n",
    "            print(\"Error line: \", line)\n",
    "    return dataset\n",
    "\n",
    "def set_seed(seed=7):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def load_hyperparam(args):\n",
    "    with codecs.open(args.config_path, \"r\", \"utf-8\") as f:\n",
    "        param = json.load(f)\n",
    "    args.emb_size = param.get(\"emb_size\", 768)\n",
    "    args.hidden_size = param.get(\"hidden_size\", 768)\n",
    "    args.kernel_size = param.get(\"kernel_size\", 3)\n",
    "    args.block_size = param.get(\"block_size\", 2)\n",
    "    args.feedforward_size = param.get(\"feedforward_size\", None)\n",
    "    args.heads_num = param.get(\"heads_num\", None)\n",
    "    args.layers_num = param.get(\"layers_num\", 12)\n",
    "    args.dropout = param.get(\"dropout\", 0.1)\n",
    "    \n",
    "    return args\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    # We dont't need prefix \"module\".\n",
    "    if hasattr(model, \"module\"):\n",
    "        torch.save(model.module.state_dict(), model_path)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {    \n",
    "    \"model_path\": \"./models/bert_origin/\",\n",
    "    \"config_path\": \"./models/bert_origin/config.json\",\n",
    "    \n",
    "#     \"train_path\": \"/input/datasets_K-BERT/nlpcc-dbqa/train.tsv\",\n",
    "#     \"dev_path\":  \"/input/datasets_K-BERT/nlpcc-dbqa/dev.tsv\",\n",
    "#     \"test_path\":  \"/input/datasets_K-BERT/nlpcc-dbqa/test.tsv\",\n",
    "#     \"output_model_path\": \"./outputs/kbert_nlpcc-dbqa_CnDbpedia.bin\",\n",
    "    \"train_path\": \"/input/datasets_K-BERT/book_review/train.tsv\",\n",
    "    \"dev_path\":  \"/input/datasets_K-BERT/book_review/dev.tsv\",\n",
    "    \"test_path\":  \"/input/datasets_K-BERT/book_review/test.tsv\",\n",
    "    \"output_model_path\": \"./outputs/kbert_book_review_CnDbpedia.bin\",\n",
    "    \n",
    "    \"kg_name\": \"CnDbpedia\",\n",
    "\n",
    "    \"batch_size\": 128, # 32, 64, 128\n",
    "    \"seq_length\": 256,\n",
    "    \"learning_rate\":2e-5 ,\n",
    "    \"warmup\": 0.1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"epochs_num\": 10, # 5, 10, 20\n",
    "    \"report_steps\": 100,\n",
    "    \"seed\": 7,\n",
    "    \"mean_reciprocal_rank\": False, # True for DBQA dataset\n",
    "    \"workers_num\": 1, # number of process for loading dataset，取决于cpu数量和线程数量\n",
    "    \"no_vm\": False, # Disable the visible_matrix\n",
    "}\n",
    "\n",
    "class Args(dict):  #字典转对象，递归版,既可以作为对象、也可以作为属性\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__\n",
    "args = Args(args)\n",
    "\n",
    "# Load the hyperparameters from the config file.\n",
    "args = load_hyperparam(args)\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Count the number of labels.\n",
    "labels_set = set()\n",
    "columns = {}\n",
    "with open(args.train_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line_id, line in enumerate(f):\n",
    "        try:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line_id == 0:\n",
    "                for i, column_name in enumerate(line):\n",
    "                    columns[column_name] = i\n",
    "                continue\n",
    "            label = int(line[columns[\"label\"]])\n",
    "            labels_set.add(label)\n",
    "        except:\n",
    "            pass\n",
    "args.labels_num = len(labels_set) \n",
    "\n",
    "# Build knowledge graph.\n",
    "if args.kg_name == 'none':\n",
    "    spo_files = []\n",
    "else:\n",
    "    spo_files = [args.kg_name]\n",
    "kg = KnowledgeGraph(spo_files=spo_files, predicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from transformers import (BertConfig, BertTokenizer, BertModel, \n",
    "                          DistilBertConfig, DistilBertTokenizer, DistilBertModel)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(args.model_path)\n",
    "bert_token = BertTokenizer.from_pretrained(args.model_path)\n",
    "bert_model = BertModel(config=bert_config)\n",
    "bert_model.config.max_position_embeddings = args.seq_length #句子最大长度256\n",
    "model = BertClassifier(args, bert_model)\n",
    "# model.from_pretrained(args.model_path)\n",
    "# model.load_state_dict(torch.load(args.model_path+'pytorch_model.bin'), strict=False)\n",
    "model.load_state_dict(torch.load('./outputs/kbert_book_review_CnDbpedia.bin'), strict=False)\n",
    "\n",
    "# For simplicity, we use DataParallel wrapper to use multiple GPUs.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"{} GPUs are available. Let's use them.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datset loader.\n",
    "def batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms):\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size]\n",
    "        seg_ids_batch = seg_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, label_ids_batch, seg_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n",
    "        seg_ids_batch = seg_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "        yield input_ids_batch, label_ids_batch, seg_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "def read_dataset(path, workers_num=1):\n",
    "\n",
    "    print(\"Loading sentences from {}\".format(path))\n",
    "    sentences = []\n",
    "    with open(path, mode='r', encoding=\"utf-8\") as f:\n",
    "        for line_id, line in enumerate(f):\n",
    "            if line_id == 0:\n",
    "                continue\n",
    "            sentences.append(line)\n",
    "    sentence_num = len(sentences)\n",
    "\n",
    "    print(\"There are {} sentence in total. We use {} processes to inject knowledge into sentences.\".format(sentence_num, workers_num))\n",
    "    if workers_num > 1:\n",
    "        params = []\n",
    "        sentence_per_block = int(sentence_num / workers_num) + 1\n",
    "        for i in range(workers_num):\n",
    "            params.append((i, sentences[i*sentence_per_block: (i+1)*sentence_per_block], columns, kg, bert_token, args.seq_length)) # modified\n",
    "        pool = Pool(workers_num)\n",
    "        res = pool.map(add_knowledge_worker, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        dataset = [sample for block in res for sample in block]\n",
    "    else:\n",
    "        params = (0, sentences, columns, kg, bert_token, args.seq_length) # modified\n",
    "        dataset = add_knowledge_worker(params)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Evaluation function.\n",
    "def evaluate(args, is_test, metrics='Acc'):\n",
    "    if is_test:\n",
    "        dataset = read_dataset(args.test_path, workers_num=args.workers_num)\n",
    "    else:\n",
    "        dataset = read_dataset(args.dev_path, workers_num=args.workers_num)\n",
    "\n",
    "    input_ids = torch.LongTensor([sample[0] for sample in dataset])\n",
    "    label_ids = torch.LongTensor([sample[1] for sample in dataset])\n",
    "    seg_ids = torch.LongTensor([sample[2] for sample in dataset])\n",
    "    pos_ids = torch.LongTensor([example[3] for example in dataset])\n",
    "    vms = [example[4] for example in dataset] # list of 2-dim tensor\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    instances_num = input_ids.size()[0]\n",
    "    if is_test:\n",
    "        print(\"The number of evaluation instances: \", instances_num)\n",
    "\n",
    "    correct = 0\n",
    "    # Confusion matrix.\n",
    "    confusion = torch.zeros(args.labels_num, args.labels_num, dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if not args.mean_reciprocal_rank:\n",
    "        for i, (input_ids_batch, label_ids_batch,  seg_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms)):\n",
    "\n",
    "            # vms_batch = vms_batch.long()\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            seg_ids_batch = seg_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    loss, logits = model(input_ids_batch, label_ids_batch, vms_batch, seg_ids_batch, pos_ids_batch)\n",
    "                except:\n",
    "                    print(input_ids_batch)\n",
    "                    print(input_ids_batch.size())\n",
    "                    print(vms_batch)\n",
    "                    print(vms_batch.size())\n",
    "\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            gold = label_ids_batch\n",
    "            for j in range(pred.size()[0]):\n",
    "                confusion[pred[j], gold[j]] += 1\n",
    "            correct += torch.sum(pred == gold).item()\n",
    "\n",
    "        if is_test:\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion)\n",
    "            print(\"Report precision, recall, and f1:\")\n",
    "\n",
    "        for i in range(confusion.size()[0]):\n",
    "            p = confusion[i,i].item()/confusion[i,:].sum().item()\n",
    "            r = confusion[i,i].item()/confusion[:,i].sum().item()\n",
    "            f1 = 2*p*r / (p+r)\n",
    "            if i == 1:\n",
    "                label_1_f1 = f1\n",
    "            print(\"Label {}: {:.3f}, {:.3f}, {:.3f}\".format(i,p,r,f1))\n",
    "        print(\"Acc. (Correct/Total): {:.4f} ({}/{}) \".format(correct/len(dataset), correct, len(dataset)))\n",
    "        if metrics == 'Acc':\n",
    "            return correct/len(dataset)\n",
    "        elif metrics == 'f1':\n",
    "            return label_1_f1\n",
    "        else:\n",
    "            return correct/len(dataset)\n",
    "    else:\n",
    "        for i, (input_ids_batch, label_ids_batch,  seg_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms)):\n",
    "\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            seg_ids_batch = seg_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss, logits = model(input_ids_batch, label_ids_batch, vms_batch, seg_ids_batch, pos_ids_batch)\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            if i == 0:\n",
    "                logits_all=logits\n",
    "            if i >= 1:\n",
    "                logits_all=torch.cat((logits_all,logits),0)\n",
    "\n",
    "        order = -1\n",
    "        gold = []\n",
    "        for i in range(len(dataset)):\n",
    "            qid = dataset[i][-1]\n",
    "            label = dataset[i][1]\n",
    "            if qid == order:\n",
    "                j += 1\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "            else:\n",
    "                order = qid\n",
    "                j = 0\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "\n",
    "        label_order = []\n",
    "        order = -1\n",
    "        for i in range(len(gold)):\n",
    "            if gold[i][0] == order:\n",
    "                templist.append(gold[i][1])\n",
    "            elif gold[i][0] != order:\n",
    "                order=gold[i][0]\n",
    "                if i > 0:\n",
    "                    label_order.append(templist)\n",
    "                templist = []\n",
    "                templist.append(gold[i][1])\n",
    "        label_order.append(templist)\n",
    "\n",
    "        order = -1\n",
    "        score_list = []\n",
    "        for i in range(len(logits_all)):\n",
    "            score = float(logits_all[i][1])\n",
    "            qid=int(dataset[i][-1])\n",
    "            if qid == order:\n",
    "                templist.append(score)\n",
    "            else:\n",
    "                order = qid\n",
    "                if i > 0:\n",
    "                    score_list.append(templist)\n",
    "                templist = []\n",
    "                templist.append(score)\n",
    "        score_list.append(templist)\n",
    "\n",
    "        rank = []\n",
    "        pred = []\n",
    "        print(len(score_list))\n",
    "        print(len(label_order))\n",
    "        for i in range(len(score_list)):\n",
    "            if len(label_order[i])==1:\n",
    "                if label_order[i][0] < len(score_list[i]):\n",
    "                    true_score = score_list[i][label_order[i][0]]\n",
    "                    score_list[i].sort(reverse=True)\n",
    "                    for j in range(len(score_list[i])):\n",
    "                        if score_list[i][j] == true_score:\n",
    "                            rank.append(1 / (j + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "\n",
    "            else:\n",
    "                true_rank = len(score_list[i])\n",
    "                for k in range(len(label_order[i])):\n",
    "                    if label_order[i][k] < len(score_list[i]):\n",
    "                        true_score = score_list[i][label_order[i][k]]\n",
    "                        temp = sorted(score_list[i],reverse=True)\n",
    "                        for j in range(len(temp)):\n",
    "                            if temp[j] == true_score:\n",
    "                                if j < true_rank:\n",
    "                                    true_rank = j\n",
    "                if true_rank < len(score_list[i]):\n",
    "                    rank.append(1 / (true_rank + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "        MRR = sum(rank) / len(rank)\n",
    "        print(\"MRR\", MRR)\n",
    "        return MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training phase.\n",
    "print(\"Start training.\")\n",
    "trainset = read_dataset(args.train_path, workers_num=args.workers_num)\n",
    "print(\"Shuffling dataset\")\n",
    "random.shuffle(trainset)\n",
    "instances_num = len(trainset)\n",
    "batch_size = args.batch_size\n",
    "\n",
    "print(\"Trans data to tensor.\")\n",
    "print(\"input_ids\")\n",
    "input_ids = torch.LongTensor([example[0] for example in trainset])\n",
    "print(\"label_ids\")\n",
    "label_ids = torch.LongTensor([example[1] for example in trainset])\n",
    "print(\"seg_ids\")\n",
    "seg_ids = torch.LongTensor([example[2] for example in trainset])\n",
    "print(\"pos_ids\")\n",
    "pos_ids = torch.LongTensor([example[3] for example in trainset])\n",
    "print(\"vms\")\n",
    "vms = [example[4] for example in trainset]\n",
    "\n",
    "train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"The number of training instances:\", instances_num)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n",
    "\n",
    "total_loss = 0.\n",
    "result = 0.0\n",
    "# best_result = 0.0\n",
    "best_result = 0.8143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training phase 2\n",
    "for epoch in range(1, args.epochs_num+1):\n",
    "    model.train()\n",
    "    for i, (input_ids_batch, label_ids_batch, seg_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        label_ids_batch = label_ids_batch.to(device)\n",
    "        seg_ids_batch = seg_ids_batch.to(device)\n",
    "        pos_ids_batch = pos_ids_batch.to(device)\n",
    "        vms_batch = vms_batch.to(device)\n",
    "\n",
    "        loss, _ = model(input_ids_batch, label_ids_batch, vms_batch, seg_ids_batch, pos=pos_ids_batch)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = torch.mean(loss)\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % args.report_steps == 0:\n",
    "            print(\"Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(epoch, i+1, total_loss / args.report_steps))\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Start evaluation on dev dataset.\")\n",
    "    result = evaluate(args, False)\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        save_model(model, args.output_model_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(\"Start evaluation on test dataset.\")\n",
    "    evaluate(args, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation phase.\n",
    "print(\"Final evaluation on the test dataset.\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(torch.load(args.output_model_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.output_model_path))\n",
    "evaluate(args, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
