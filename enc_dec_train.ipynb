{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### !/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import collections\n",
    "import codecs\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List\n",
    "from multiprocessing import Process, Pool\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (BertConfig, BertTokenizer, \n",
    "                          BertModel, BertPreTrainedModel,\n",
    "                          DistilBertConfig, DistilBertTokenizer,\n",
    "                          GPT2Config, GPT2LMHeadModel,\n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "\n",
    "from modeling_distilbert import (DistilBertModel, \n",
    "                                 DistilBertPreTrainedModel)\n",
    "\n",
    "from uer.optimizers import BertAdam\n",
    "from brain import KnowledgeGraph\n",
    "from utils import (set_seed, create_logger, save_model, \n",
    "                    calculate_loss_and_accuracy)\n",
    "from constants import * \n",
    "\n",
    "# arguments setting\n",
    "def load_hyperparam(args):\n",
    "    with codecs.open(args.enc_config_path, \"r\", \"utf-8\") as f:\n",
    "        param = json.load(f)\n",
    "    args.emb_size = param.get(\"emb_size\", 768)\n",
    "    args.hidden_size = param.get(\"hidden_size\", 768)\n",
    "    args.kernel_size = param.get(\"kernel_size\", 3)\n",
    "    args.block_size = param.get(\"block_size\", 2)\n",
    "    args.feedforward_size = param.get(\"feedforward_size\", None)\n",
    "    args.heads_num = param.get(\"heads_num\", None)\n",
    "    args.layers_num = param.get(\"layers_num\", 12)\n",
    "    args.dropout = param.get(\"dropout\", 0.1)\n",
    "    \n",
    "    return args\n",
    "\n",
    "args = {\n",
    "    \"enc_model_path\": \"./models/distilbert-chinese/\",\n",
    "    \"enc_config_path\": \"./models/distilbert-chinese/config.json\",\n",
    "    \"dec_model_path\": \"./models/gpt2_dialogue_model/\",\n",
    "    \"dec_config_path\": \"./models/gpt2_dialogue_model/config.json\",\n",
    "    \n",
    "    \"train_path\": \"/input/datasets_K-BERT/STC-corpus/STC.json\",\n",
    "#     \"dev_path\":  \"/input/datasets_K-BERT/book_review/dev.tsv\",\n",
    "    \"test_path\":  \"/input/datasets_K-BERT/STC-corpus/STC_test.json\",\n",
    "\n",
    "    \"model_output_path\": \"./outputs/encdec_STC_CnDbpedia.bin\",    \n",
    "    \"kg_name\": \"CnDbpedia\",\n",
    "    \"log_path\": \"/output/enc_dec_log.txt\",\n",
    "    \"tb_writer_dir\": \"/output\",\n",
    "\n",
    "    \"batch_size\": 32, # 32, 64, 128\n",
    "    \"seq_length\": 256,\n",
    "    \"learning_rate\":2e-5 , # 2e-5, 5e-5\n",
    "    \"warmup\": 0.1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"epochs_num\": 2, # 5, 10, 20\n",
    "    \"log_step\": 10, #多少步汇报一次loss\n",
    "    \"max_grad_norm\": 1.0, #梯度裁剪\n",
    "    \"gradient_accumulation\": 2,  #每n次反向传播/batch，进行一次梯度下降\n",
    "    \"seed\": 7,\n",
    "    \"mean_reciprocal_rank\": False, # True for DBQA dataset\n",
    "    \"workers_num\": 1, # number of process for loading dataset，取决于cpu数量和线程数量\n",
    "    \"no_vm\": False, # Disable the visible_matrix\n",
    "}\n",
    "\n",
    "class Args(dict):  #字典转对象，递归版,既可以作为对象、也可以作为属性\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__\n",
    "args = Args(args)\n",
    "args = load_hyperparam(args) # Load the hyperparameters from the config file.\n",
    "\n",
    "# basic setting\n",
    "logger = create_logger(args)\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build knowledge graph.\n",
    "if args.kg_name == 'none':\n",
    "    spo_files = []\n",
    "else:\n",
    "    spo_files = [args.kg_name]\n",
    "kg = KnowledgeGraph(spo_files=spo_files, predicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "## whole model\n",
    "class EncDecModel(transformers.PreTrainedModel):\n",
    "    def __init__(self,config, args, enc_model, dec_model):\n",
    "        super().__init__(config)\n",
    "        self.enc = enc_model\n",
    "        self.dec = dec_model\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None, \n",
    "        token_type_ids=None, \n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=True,\n",
    "        label_ids = None\n",
    "    ):\n",
    "        \n",
    "        enc_output = self.enc(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids, \n",
    "            token_type_ids=token_type_ids, \n",
    "            head_mask=None,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        enc_hidden_states = enc_output[0]\n",
    "        label_embeds = self.dec.transformer.wte(label_ids) # id2embedding\n",
    "        label_embeds[:,0] = enc_hidden_states[:,0] # 只替换[CLS]\n",
    "        outputs = self.dec(inputs_embeds=label_embeds)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoder distilbert origin\n",
    "enc_model_config = DistilBertConfig.from_pretrained(args.enc_model_path)\n",
    "enc_model_token = BertTokenizer.from_pretrained(args.enc_model_path)\n",
    "enc_model = DistilBertModel(config=enc_model_config)\n",
    "enc_model.config.max_position_embeddings = args.seq_length #句子最大长度256\n",
    "\n",
    "# module_dict = torch.load(args.model_path+'pytorch_model.bin')\n",
    "module_dict = torch.load(\"./outputs/distilbert_book_review_CnDbpedia.bin\")\n",
    "new_module_dict = OrderedDict()\n",
    "# 处理 导入module_dict名称不匹配问题\n",
    "for layer in module_dict.keys():\n",
    "    if layer[:6] == 'origin':\n",
    "        new_module_dict[layer[7:]] = module_dict[layer]\n",
    "    else:\n",
    "        new_module_dict[layer] = module_dict[layer]\n",
    "enc_model.load_state_dict(new_module_dict, strict=False)\n",
    "\n",
    "## decoder model\n",
    "dec_model_token = BertTokenizer.from_pretrained(args.dec_model_path)\n",
    "dec_model = GPT2LMHeadModel.from_pretrained(args.dec_model_path)\n",
    "model = EncDecModel(config=enc_model_config, args=args,enc_model=enc_model, dec_model=dec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_knowledge_worker(params):\n",
    "    p_id, sentences, kg, enc_tokenizer, dec_tokenizer, max_length = params # modified\n",
    "    sentences_num = len(sentences)\n",
    "    dataset = []\n",
    "    assert len(sentences[0]) == 2\n",
    "    for line_id, line in enumerate(sentences):\n",
    "        if line_id % 10000 == 0:\n",
    "            print(\"Progress of process {}: {}/{}\".format(p_id, line_id, sentences_num))\n",
    "            sys.stdout.flush()\n",
    "        try: \n",
    "            text_a, text_b = line[0], line[1]\n",
    "            text = CLS_TOKEN + text_a\n",
    "            tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=max_length) # modified\n",
    "            tokens = tokens[0]\n",
    "            pos = pos[0]\n",
    "            vm = vm[0]\n",
    "            token_ids = enc_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            text_dec = CLS_TOKEN + text_b\n",
    "            text_dec = dec_tokenizer.tokenize(text_dec) # str2list, 自带数据清洗和特殊标志识别\n",
    "            text_dec = dec_tokenizer.convert_tokens_to_ids(text_dec) # list of str -> list of id\n",
    "            pad_num = max_length - len(text_dec)\n",
    "            if pad_num > 0:\n",
    "                text_dec.extend([PAD_ID] * pad_num)\n",
    "            else:\n",
    "                text_dec = text_dec[:max_length]\n",
    "\n",
    "            seg = []\n",
    "            seg_tag = 0\n",
    "            for t in tokens:\n",
    "                seg.append(seg_tag)\n",
    "                if t == SEP_TOKEN:\n",
    "                    seg_tag += 1\n",
    "\n",
    "            dataset.append((token_ids, text_dec, seg, pos, vm))\n",
    "        except:\n",
    "            print(f\"Error line: {line_id}, {line}\")\n",
    "    return dataset\n",
    "\n",
    "# Datset loader.\n",
    "def batch_loader(batch_size, input_ids, text_dec, seg_ids, pos_ids, vms):\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        text_dec_batch = text_dec[i*batch_size: (i+1)*batch_size]\n",
    "        seg_ids_batch = seg_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, text_dec_batch, seg_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        text_dec_batch = text_dec[instances_num//batch_size*batch_size:]\n",
    "        seg_ids_batch = seg_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "        yield input_ids_batch, text_dec_batch, seg_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "## read dataset\n",
    "sentences = []\n",
    "with open(args.test_path, mode='r', encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    sentences = data['test']\n",
    "\n",
    "print(\"There are {} sentence in total. We use {} processes to inject knowledge into sentences.\".format(len(sentences), 1))\n",
    "dataset = add_knowledge_worker((0, sentences, kg, enc_model_token, dec_model_token, args.seq_length))\n",
    "\n",
    "## 处理dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "logger.info(\"Trans data to tensor.\")\n",
    "logger.info(\"input_ids\")\n",
    "input_ids = torch.LongTensor([example[0] for example in dataset])\n",
    "logger.info(\"text_dec\")\n",
    "text_dec = torch.LongTensor([example[1] for example in dataset])\n",
    "logger.info(\"seg_ids\")\n",
    "seg_ids = torch.LongTensor([example[2] for example in dataset])\n",
    "logger.info(\"pos_ids\")\n",
    "pos_ids = torch.LongTensor([example[3] for example in dataset])\n",
    "logger.info(\"vms\")\n",
    "vms = [example[4] for example in dataset]\n",
    "\n",
    "instances_num = len(dataset)\n",
    "train_steps = int(instances_num * args.epochs_num / args.batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型并行化\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    logging.info(\"{} GPUs are available. Let's use them.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model) #并行化\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "## 记录模型参数数量\n",
    "num_parameters = 0\n",
    "\n",
    "parameters = model.parameters()\n",
    "for parameter in parameters:\n",
    "    num_parameters += parameter.numel()\n",
    "logger.info('number of model parameters: {}'.format(num_parameters))\n",
    "\n",
    "##建立优化器和学习率调度器\n",
    "t_total = len(dataset) // args.gradient_accumulation * args.epochs_num # t_total = batch_num * epochs，代表实际梯度下降总次数\n",
    "warmup_steps = t_total // 2 #和学习率调整有关\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=args.learning_rate) #梯度优化器\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total) #倾斜三角式，学习率调度器\n",
    "\n",
    "## 创建对话模型的输出目录\n",
    "if not os.path.exists(args.model_output_path):\n",
    "    os.mkdir(args.model_output_path)\n",
    "\n",
    "##开始训练\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num Epochs = {args.epochs_num}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {args.batch_size * args.gradient_accumulation}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation}\")\n",
    "logger.info(f\"  Total optimization steps = {t_total}\")\n",
    "logger.info(f\"  Batch size: {args.batch_size}\")\n",
    "logger.info(f\"  The number of training instances: {instances_num}\")\n",
    "logger.info('starting training')\n",
    "# 用于统计每次梯度累计的loss\n",
    "running_loss = 0\n",
    "# 统计一共训练了多少个step\n",
    "overall_step = 0\n",
    "# 记录tensorboardX\n",
    "tb_writer = SummaryWriter(log_dir=args.tb_writer_dir)\n",
    "# 记录 out of memory的次数\n",
    "oom_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "for epoch in range(args.epochs_num):\n",
    "    epoch_start_time = datetime.now()\n",
    "    model.train()\n",
    "    for batch_idx, (input_ids_batch, text_dec_batch, seg_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(args.batch_size, input_ids, text_dec, seg_ids, pos_ids, vms)):\n",
    "        model.zero_grad()\n",
    "        # 注意：GPT2模型的forward()函数，是对于给定的context，生成一个token，而不是生成一串token\n",
    "        # GPT2Model的输入为n个token_id时，输出也是n个hidden_state，使用第n个hidden_state预测第n+1个token\n",
    "        vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        text_dec_batch = text_dec_batch.to(device)\n",
    "        seg_ids_batch = seg_ids_batch.to(device)\n",
    "        pos_ids_batch = pos_ids_batch.to(device)\n",
    "        vms_batch = vms_batch.to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids_batch, \n",
    "            attention_mask=vms_batch, \n",
    "            position_ids=pos_ids_batch,\n",
    "            token_type_ids=seg_ids_batch, \n",
    "            label_ids=text_dec_batch\n",
    "        )\n",
    "        loss, accuracy = calculate_loss_and_accuracy(outputs, labels=text_dec_batch, device=device)\n",
    "        loss = loss.mean()\n",
    "        accuracy = accuracy.mean()\n",
    "        if args.gradient_accumulation > 1:\n",
    "            loss = loss / args.gradient_accumulation\n",
    "            accuracy = accuracy / args.gradient_accumulation\n",
    "        loss.backward()\n",
    "        # 梯度裁剪解决的是梯度消失或爆炸的问题，即设定阈值\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        # 进行一定step的梯度累计之后，更新参数\n",
    "        if (batch_idx + 1) % args.gradient_accumulation == 0:\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step() # 更新参数，进行梯度下降\n",
    "            optimizer.zero_grad() # 清空梯度信息\n",
    "            scheduler.step() # 进行warm up，调整学习率\n",
    "            overall_step += 1\n",
    "            # 更新日志与tnesorboardX信息\n",
    "            if (overall_step + 1) % args.log_step == 0:\n",
    "                logger.info(\n",
    "                    \"batch {} of epoch {}, loss {}, accuracy {}\".format(batch_idx + 1, epoch + 1, loss,\n",
    "                                                                        accuracy))\n",
    "                tb_writer.add_scalar('loss', loss.item(), overall_step)\n",
    "    logger.info('saving model for epoch {}'.format(epoch + 1))\n",
    "\n",
    "    model_path = join(args.model_output_path, 'model_epoch{}'.format(epoch + 1))\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(model_path)\n",
    "    logger.info('epoch {} finished'.format(epoch + 1))\n",
    "    epoch_finish_time = datetime.now()\n",
    "    logger.info('time for one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n",
    "logger.info('training finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
