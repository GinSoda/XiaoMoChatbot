{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "from itertools import chain\n",
    "from argparse import ArgumentParser\n",
    "from pprint import pformat\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import OpenAIGPTLMHeadModel, GPT2LMHeadModel, BertTokenizer\n",
    "\n",
    "SPECIAL_TOKENS = [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[speaker1]\", \"[speaker2]\"]\n",
    "\n",
    "\n",
    "def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def build_input_from_segments(history, reply, tokenizer, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply \"\"\"\n",
    "    bos, eos, pad, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "#     print(\"1\", sequence)\n",
    "    sequence = [sequence[0]] + [[speaker2 if i % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "#     print(\"2\", sequence)\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [bos] + [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence[1:])\n",
    "                                          for _ in s]\n",
    "    return instance, sequence\n",
    "\n",
    "\n",
    "def sample_sequence(history, tokenizer, model, args, current_output=None):\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    if current_output is None:\n",
    "        current_output = []\n",
    "\n",
    "    for i in range(args.max_length):\n",
    "        instance, _ = build_input_from_segments(history, current_output, tokenizer, with_eos=False)\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], dtype=torch.long, device=args.device).unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], dtype=torch.long, device=args.device).unsqueeze(0)\n",
    "\n",
    "        logits, *_ = model(input_ids, token_type_ids=token_type_ids)\n",
    "        logits = logits[0, -1, :] / args.temperature\n",
    "        logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n",
    "        if i < args.min_length and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "\n",
    "    return current_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"gpt\": True,\n",
    "#     \"model_checkpoint\": \"./models/CDial-GPT2_LCCC-base\",\n",
    "    \"model_checkpoint\": \"./models/CDial-GPT_LCCC-large\",\n",
    "    \"max_history\": 2,\n",
    "    \"device\": \"cuda\",\n",
    "    \"no_sample\": True,\n",
    "    \"max_length\": 30,\n",
    "    \"min_length\": 1,\n",
    "    \"seed\": 42,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\":  0,\n",
    "    \"top_p\":  0.9,\n",
    "    \"log_path\": \"/output/CDial_log.txt\"\n",
    "}\n",
    "class Args(dict):  #字典转对象，递归版,既可以作为对象、也可以作为属性\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__\n",
    "args = Args(args)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(pformat(args))\n",
    "\n",
    "if args.model_checkpoint == \"\":\n",
    "    logging.error(\"Checkpoint needed!\")\n",
    "\n",
    "random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "logger.info(\"Get pretrained model and tokenizer\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_checkpoint, do_lower_case=True)\n",
    "model_class = OpenAIGPTLMHeadModel if not args.gpt2 else GPT2LMHeadModel\n",
    "model = model_class.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "model.to(args.device)\n",
    "model.eval()\n",
    "\n",
    "def tokenize(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    if isinstance(obj, dict):\n",
    "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "    return list(tokenize(o) for o in obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "while True:\n",
    "    raw_text = input(\">>> \")\n",
    "    while not raw_text:\n",
    "        print('Prompt should not be empty!')\n",
    "        raw_text = input(\">>> \")\n",
    "    raw_text = \" \".join(list(raw_text.replace(\" \", \"\")))\n",
    "    history.append(tokenize(raw_text))\n",
    "    with torch.no_grad():\n",
    "        out_ids = sample_sequence(history, tokenizer, model, args)\n",
    "    history.append(out_ids)\n",
    "    history = history[-(2 * args.max_history + 1):]\n",
    "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "    print(out_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
