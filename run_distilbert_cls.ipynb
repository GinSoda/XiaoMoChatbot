{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# -*- encoding:utf-8 -*-\n",
    "\"\"\"\n",
    "  This script provides an k-BERT exmaple for classification.\n",
    "\"\"\"\n",
    "import os\n",
    "import collections\n",
    "import codecs\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List\n",
    "from multiprocessing import Process, Pool\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (BertConfig, BertTokenizer, \n",
    "                          BertModel, BertPreTrainedModel,\n",
    "                          DistilBertConfig, DistilBertTokenizer)\n",
    "from modeling_distilbert import (DistilBertModel, \n",
    "                                 DistilBertPreTrainedModel)\n",
    "\n",
    "from uer.optimizers import BertAdam\n",
    "from brain import KnowledgeGraph\n",
    "from utils import set_seed, load_hyperparam, save_model\n",
    "from constants import * \n",
    "\n",
    "# student task model: DistilModel\n",
    "class DistilbertClassifierModel(DistilBertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    DistilbertClassifierModel\n",
    "\n",
    "    Distil model class based on huggingface class but with\n",
    "    initialization in it. Model will take vocabulary\n",
    "    layers from specified teacher model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config, args, origin_model,\n",
    "        layers: List[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers: layers indexes to initialize\n",
    "            extract: bool flag, if you want to initialize your model with\n",
    "                layers of the teacher model then set this to true\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "   \n",
    "        self.layers = [0, 2, 4, 7, 9, 11]\n",
    "        self.labels_num = args.labels_num\n",
    "        self.origin = origin_model #载入一个distilBert模型(无task layer 无state_dict)                                         \n",
    "        self.pre_classifier = nn.Linear(args.hidden_size, args.hidden_size)\n",
    "        self.classifier = nn.Linear(args.hidden_size, args.labels_num)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "        self.init_weights()\n",
    "        if layers is not None:\n",
    "            self.layers = layers\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None, \n",
    "        token_type_ids=None, \n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=True,\n",
    "        label_ids = None\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        distilbert_output = self.origin(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids, \n",
    "            token_type_ids=token_type_ids, \n",
    "            head_mask=None,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "        loss = self.criterion(self.softmax(logits.view(-1, self.labels_num)), label_ids.view(-1))\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract(\n",
    "        teacher_model,\n",
    "        layers: List[int]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extracts state dict from teacher model\n",
    "\n",
    "        Args:\n",
    "            teacher_model: model to extract\n",
    "            layers: layers indexes to initialize\n",
    "            prefix_teacher: name of the teacher model\n",
    "            prefix_student: name of the student model\n",
    "        \"\"\"\n",
    "        state_dict = teacher_model.state_dict()\n",
    "        compressed_sd = {}\n",
    "\n",
    "        # extract embeddings\n",
    "        compressed_sd[\"embeddings.position_ids\"] = state_dict[\"embeddings.position_ids\"]\n",
    "        for w in [\"word_embeddings\", \"position_embeddings\", \"token_type_embeddings\"]:\n",
    "            compressed_sd[\n",
    "                f\"embeddings.{w}.weight\"\n",
    "            ] = state_dict[f\"embeddings.{w}.weight\"]\n",
    "        for w in [\"weight\", \"bias\"]:\n",
    "            compressed_sd[\n",
    "                f\"embeddings.LayerNorm.{w}\"\n",
    "            ] = state_dict[f\"embeddings.LayerNorm.{w}\"]\n",
    "        # extract encoder\n",
    "        std_idx = 0\n",
    "        for teacher_idx in layers:\n",
    "            for w in [\"weight\", \"bias\"]:\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.q_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.self.query.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.k_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.self.key.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.v_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.self.value.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.out_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.output.dense.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.sa_layer_norm.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.output.LayerNorm.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.ffn.lin1.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.intermediate.dense.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.ffn.lin2.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.output.dense.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.output_layer_norm.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.output.LayerNorm.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "            std_idx += 1\n",
    "        return compressed_sd\n",
    "\n",
    "def add_knowledge_worker(params):\n",
    "    p_id, sentences, columns, kg, bert_token, max_length = params # modified\n",
    "    sentences_num = len(sentences)\n",
    "    dataset = []\n",
    "    for line_id, line in enumerate(sentences):\n",
    "        if line_id % 10000 == 0:\n",
    "            print(\"Progress of process {}: {}/{}\".format(p_id, line_id, sentences_num))\n",
    "            sys.stdout.flush()\n",
    "        line = line.strip().split('\\t')\n",
    "        try:\n",
    "            label = int(line[columns[\"label\"]])\n",
    "            if len(line) == 2:\n",
    "                text = CLS_TOKEN + line[columns[\"text_a\"]]\n",
    "   \n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=max_length) # modified\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0]\n",
    "                token_ids = bert_token.convert_tokens_to_ids(tokens)\n",
    "\n",
    "                seg = [0 for t in tokens]\n",
    "                \n",
    "                dataset.append((token_ids, label, seg, pos, vm))\n",
    "            \n",
    "            elif len(line)==3:                  \n",
    "                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n",
    "                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=max_length) # modified\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0]\n",
    "                token_ids = bert_token.convert_tokens_to_ids(tokens)\n",
    "\n",
    "                seg = []\n",
    "                seg_tag = 0\n",
    "                for t in tokens:\n",
    "                    seg.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "                \n",
    "                dataset.append((token_ids, label, seg, pos, vm))\n",
    "            \n",
    "            elif len(line) == 4:  # for dbqa\n",
    "                qid=int(line[columns[\"qid\"]])\n",
    "                text_a, text_b = line[columns[\"text_a\"]], line[columns[\"text_b\"]]\n",
    "                text = CLS_TOKEN + text_a + SEP_TOKEN + text_b + SEP_TOKEN\n",
    "\n",
    "                tokens, pos, vm, _ = kg.add_knowledge_with_vm([text], add_pad=True, max_length=max_length) # modified\n",
    "                tokens = tokens[0]\n",
    "                pos = pos[0]\n",
    "                vm = vm[0]\n",
    "                token_ids = bert_token.convert_tokens_to_ids(tokens)\n",
    "\n",
    "                seg = []\n",
    "                seg_tag = 0\n",
    "                for t in tokens:\n",
    "                    seg.append(seg_tag)\n",
    "                    if t == SEP_TOKEN:\n",
    "                        seg_tag += 1\n",
    "                \n",
    "                dataset.append((token_ids, label, seg, pos, vm, qid))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        except:\n",
    "            print(\"Error line: \", line)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {    \n",
    "    \"teacher_model_path\": \"./models/bert_origin/\",\n",
    "    \"teacher_config_path\": \"./models/bert_origin/config.json\",\n",
    "    \"model_path\": \"./models/distilbert-chinese/\",\n",
    "    \"config_path\": \"./models/distilbert-chinese/config.json\",\n",
    "    \n",
    "#     \"train_path\": \"/input/datasets_K-BERT/nlpcc-dbqa/train.tsv\",\n",
    "#     \"dev_path\":  \"/input/datasets_K-BERT/nlpcc-dbqa/dev.tsv\",\n",
    "#     \"test_path\":  \"/input/datasets_K-BERT/nlpcc-dbqa/test.tsv\",\n",
    "#     \"output_model_path\": \"./outputs/kbert_nlpcc-dbqa_CnDbpedia.bin\",\n",
    "    \"train_path\": \"/input/datasets_K-BERT/weibo/train.tsv\",\n",
    "    \"dev_path\":  \"/input/datasets_K-BERT/weibo/dev.tsv\",\n",
    "    \"test_path\":  \"/input/datasets_K-BERT/weibo/test.tsv\",\n",
    "    \"output_model_path\": \"./outputs/distilbert_book_review_CnDbpedia.bin\",\n",
    "    \n",
    "    \"kg_name\": \"CnDbpedia\",\n",
    "\n",
    "    \"batch_size\": 128, # 32, 64, 128\n",
    "    \"seq_length\": 256,\n",
    "    \"learning_rate\":2e-5 ,\n",
    "    \"warmup\": 0.1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"epochs_num\": 10, # 5, 10, 20\n",
    "    \"report_steps\": 100,\n",
    "    \"seed\": 7,\n",
    "    \"mean_reciprocal_rank\": False, # True for DBQA dataset\n",
    "    \"workers_num\": 1, # number of process for loading dataset，取决于cpu数量和线程数量\n",
    "    \"no_vm\": False, # Disable the visible_matrix\n",
    "}\n",
    "\n",
    "class Args(dict):  #字典转对象，递归版,既可以作为对象、也可以作为属性\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__\n",
    "args = Args(args)\n",
    "\n",
    "# Load the hyperparameters from the config file.\n",
    "args = load_hyperparam(args)\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Count the number of labels.\n",
    "labels_set = set()\n",
    "columns = {}\n",
    "with open(args.train_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line_id, line in enumerate(f):\n",
    "        try:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line_id == 0:\n",
    "                for i, column_name in enumerate(line):\n",
    "                    columns[column_name] = i\n",
    "                continue\n",
    "            label = int(line[columns[\"label\"]])\n",
    "            labels_set.add(label)\n",
    "        except:\n",
    "            pass\n",
    "args.labels_num = len(labels_set) \n",
    "\n",
    "# Build knowledge graph.\n",
    "if args.kg_name == 'none':\n",
    "    spo_files = []\n",
    "else:\n",
    "    spo_files = [args.kg_name]\n",
    "kg = KnowledgeGraph(spo_files=spo_files, predicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## teacher origin model\n",
    "bert_config = BertConfig.from_pretrained(args.teacher_model_path)\n",
    "bert_token = BertTokenizer.from_pretrained(args.teacher_model_path)\n",
    "bert_model = BertModel(config=bert_config)\n",
    "bert_model.config.max_position_embeddings = args.seq_length #句子最大长度256\n",
    "bert_model.load_state_dict(torch.load(args.teacher_model_path+'pytorch_model.bin'), strict=False)\n",
    "\n",
    "## student origin model(model without task layer)\n",
    "distilbert_config = DistilBertConfig.from_pretrained(args.model_path)\n",
    "distilbert_token = bert_token #这俩在transformers库里其实一个样\n",
    "distilbert_model = DistilBertModel(config=distilbert_config)\n",
    "distilbert_model.config.max_position_embeddings = args.seq_length #句子最大长度256\n",
    "\n",
    "## student task model\n",
    "model = DistilbertClassifierModel(distilbert_config, args, distilbert_model)\n",
    "# model.load_state_dict(torch.load(args.model_path+'pytorch_model.bin'), strict=False)\n",
    "model.load_state_dict(torch.load(args.output_model_path), strict=False)\n",
    "\n",
    "# extract teacher model to model.origin(model without task layer), just run once\n",
    "# distil_sd = model._extract(bert_model, model.layers)\n",
    "# model.origin.load_state_dict(distil_sd, strict=False)\n",
    "# save_model(model.origin, args.output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in distil_sd.keys():\n",
    "#     print(i)\n",
    "# for i in model.state_dict().keys():\n",
    "#     print(i)\n",
    "# [print(i) for i in bert_model.state_dict().keys()]\n",
    "# [print(i) for i in distilbert_model.state_dict().keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we use DataParallel wrapper to use multiple GPUs.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"{} GPUs are available. Let's use them.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datset loader.\n",
    "def batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms):\n",
    "    instances_num = input_ids.size()[0]\n",
    "    for i in range(instances_num // batch_size):\n",
    "        input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size]\n",
    "        seg_ids_batch = seg_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        pos_ids_batch = pos_ids[i*batch_size: (i+1)*batch_size, :]\n",
    "        vms_batch = vms[i*batch_size: (i+1)*batch_size]\n",
    "        yield input_ids_batch, label_ids_batch, seg_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "    if instances_num > instances_num // batch_size * batch_size:\n",
    "        input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n",
    "        label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n",
    "        seg_ids_batch = seg_ids[instances_num//batch_size*batch_size:, :]\n",
    "        pos_ids_batch = pos_ids[instances_num//batch_size*batch_size:, :]\n",
    "        vms_batch = vms[instances_num//batch_size*batch_size:]\n",
    "        yield input_ids_batch, label_ids_batch, seg_ids_batch, pos_ids_batch, vms_batch\n",
    "\n",
    "def read_dataset(path, workers_num=1):\n",
    "\n",
    "    print(\"Loading sentences from {}\".format(path))\n",
    "    sentences = []\n",
    "    with open(path, mode='r', encoding=\"utf-8\") as f:\n",
    "        for line_id, line in enumerate(f):\n",
    "            if line_id == 0:\n",
    "                continue\n",
    "            sentences.append(line)\n",
    "    sentence_num = len(sentences)\n",
    "\n",
    "    print(\"There are {} sentence in total. We use {} processes to inject knowledge into sentences.\".format(sentence_num, workers_num))\n",
    "    if workers_num > 1:\n",
    "        params = []\n",
    "        sentence_per_block = int(sentence_num / workers_num) + 1\n",
    "        for i in range(workers_num):\n",
    "            params.append((i, sentences[i*sentence_per_block: (i+1)*sentence_per_block], columns, kg, bert_token, args.seq_length)) # modified\n",
    "        pool = Pool(workers_num)\n",
    "        res = pool.map(add_knowledge_worker, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        dataset = [sample for block in res for sample in block]\n",
    "    else:\n",
    "        params = (0, sentences, columns, kg, bert_token, args.seq_length) # modified\n",
    "        dataset = add_knowledge_worker(params)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Evaluation function.\n",
    "def evaluate(args, is_test, metrics='Acc'):\n",
    "    if is_test:\n",
    "        dataset = read_dataset(args.test_path, workers_num=args.workers_num)\n",
    "    else:\n",
    "        dataset = read_dataset(args.dev_path, workers_num=args.workers_num)\n",
    "\n",
    "    input_ids = torch.LongTensor([sample[0] for sample in dataset])\n",
    "    label_ids = torch.LongTensor([sample[1] for sample in dataset])\n",
    "    seg_ids = torch.LongTensor([sample[2] for sample in dataset])\n",
    "    pos_ids = torch.LongTensor([example[3] for example in dataset])\n",
    "    vms = [example[4] for example in dataset] # list of 2-dim tensor\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    instances_num = input_ids.size()[0]\n",
    "    if is_test:\n",
    "        print(\"The number of evaluation instances: \", instances_num)\n",
    "\n",
    "    correct = 0\n",
    "    # Confusion matrix.\n",
    "    confusion = torch.zeros(args.labels_num, args.labels_num, dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if not args.mean_reciprocal_rank:\n",
    "        for i, (input_ids_batch, label_ids_batch,  seg_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms)):\n",
    "\n",
    "            # vms_batch = vms_batch.long()\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            seg_ids_batch = seg_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    loss, logits = model(\n",
    "                        input_ids=input_ids_batch, \n",
    "                        attention_mask=vms_batch, \n",
    "                        position_ids=pos_ids_batch,\n",
    "                        token_type_ids=seg_ids_batch, \n",
    "                        label_ids=label_ids_batch\n",
    "                     )\n",
    "                except:\n",
    "                    print(input_ids_batch)\n",
    "                    print(input_ids_batch.size())\n",
    "                    print(vms_batch)\n",
    "                    print(vms_batch.size())\n",
    "\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            gold = label_ids_batch\n",
    "            for j in range(pred.size()[0]):\n",
    "                confusion[pred[j], gold[j]] += 1\n",
    "            correct += torch.sum(pred == gold).item()\n",
    "\n",
    "        if is_test:\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion)\n",
    "            print(\"Report precision, recall, and f1:\")\n",
    "\n",
    "        for i in range(confusion.size()[0]):\n",
    "            p = confusion[i,i].item()/confusion[i,:].sum().item()\n",
    "            r = confusion[i,i].item()/confusion[:,i].sum().item()\n",
    "            f1 = 2*p*r / (p+r)\n",
    "            if i == 1:\n",
    "                label_1_f1 = f1\n",
    "            print(\"Label {}: {:.3f}, {:.3f}, {:.3f}\".format(i,p,r,f1))\n",
    "        print(\"Acc. (Correct/Total): {:.4f} ({}/{}) \".format(correct/len(dataset), correct, len(dataset)))\n",
    "        if metrics == 'Acc':\n",
    "            return correct/len(dataset)\n",
    "        elif metrics == 'f1':\n",
    "            return label_1_f1\n",
    "        else:\n",
    "            return correct/len(dataset)\n",
    "    else:\n",
    "        for i, (input_ids_batch, label_ids_batch,  seg_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms)):\n",
    "\n",
    "            vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            label_ids_batch = label_ids_batch.to(device)\n",
    "            seg_ids_batch = seg_ids_batch.to(device)\n",
    "            pos_ids_batch = pos_ids_batch.to(device)\n",
    "            vms_batch = vms_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss, logits = model(\n",
    "                    input_ids=input_ids_batch, \n",
    "                    attention_mask=vms_batch, \n",
    "                    position_ids=pos_ids_batch,\n",
    "                    token_type_ids=seg_ids_batch, \n",
    "                    label_ids=label_ids_batch\n",
    "                 )\n",
    "            logits = nn.Softmax(dim=1)(logits)\n",
    "            if i == 0:\n",
    "                logits_all=logits\n",
    "            if i >= 1:\n",
    "                logits_all=torch.cat((logits_all,logits),0)\n",
    "\n",
    "        order = -1\n",
    "        gold = []\n",
    "        for i in range(len(dataset)):\n",
    "            qid = dataset[i][-1]\n",
    "            label = dataset[i][1]\n",
    "            if qid == order:\n",
    "                j += 1\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "            else:\n",
    "                order = qid\n",
    "                j = 0\n",
    "                if label == 1:\n",
    "                    gold.append((qid,j))\n",
    "\n",
    "        label_order = []\n",
    "        order = -1\n",
    "        for i in range(len(gold)):\n",
    "            if gold[i][0] == order:\n",
    "                templist.append(gold[i][1])\n",
    "            elif gold[i][0] != order:\n",
    "                order=gold[i][0]\n",
    "                if i > 0:\n",
    "                    label_order.append(templist)\n",
    "                templist = []\n",
    "                templist.append(gold[i][1])\n",
    "        label_order.append(templist)\n",
    "\n",
    "        order = -1\n",
    "        score_list = []\n",
    "        for i in range(len(logits_all)):\n",
    "            score = float(logits_all[i][1])\n",
    "            qid=int(dataset[i][-1])\n",
    "            if qid == order:\n",
    "                templist.append(score)\n",
    "            else:\n",
    "                order = qid\n",
    "                if i > 0:\n",
    "                    score_list.append(templist)\n",
    "                templist = []\n",
    "                templist.append(score)\n",
    "        score_list.append(templist)\n",
    "\n",
    "        rank = []\n",
    "        pred = []\n",
    "        print(len(score_list))\n",
    "        print(len(label_order))\n",
    "        for i in range(len(score_list)):\n",
    "            if len(label_order[i])==1:\n",
    "                if label_order[i][0] < len(score_list[i]):\n",
    "                    true_score = score_list[i][label_order[i][0]]\n",
    "                    score_list[i].sort(reverse=True)\n",
    "                    for j in range(len(score_list[i])):\n",
    "                        if score_list[i][j] == true_score:\n",
    "                            rank.append(1 / (j + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "\n",
    "            else:\n",
    "                true_rank = len(score_list[i])\n",
    "                for k in range(len(label_order[i])):\n",
    "                    if label_order[i][k] < len(score_list[i]):\n",
    "                        true_score = score_list[i][label_order[i][k]]\n",
    "                        temp = sorted(score_list[i],reverse=True)\n",
    "                        for j in range(len(temp)):\n",
    "                            if temp[j] == true_score:\n",
    "                                if j < true_rank:\n",
    "                                    true_rank = j\n",
    "                if true_rank < len(score_list[i]):\n",
    "                    rank.append(1 / (true_rank + 1))\n",
    "                else:\n",
    "                    rank.append(0)\n",
    "        MRR = sum(rank) / len(rank)\n",
    "        print(\"MRR\", MRR)\n",
    "        return MRR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training phase.\n",
    "print(\"Start training.\")\n",
    "trainset = read_dataset(args.train_path, workers_num=args.workers_num)\n",
    "print(\"Shuffling dataset\")\n",
    "random.shuffle(trainset)\n",
    "instances_num = len(trainset)\n",
    "batch_size = args.batch_size\n",
    "\n",
    "print(\"Trans data to tensor.\")\n",
    "print(\"input_ids\")\n",
    "input_ids = torch.LongTensor([example[0] for example in trainset])\n",
    "print(\"label_ids\")\n",
    "label_ids = torch.LongTensor([example[1] for example in trainset])\n",
    "print(\"seg_ids\")\n",
    "seg_ids = torch.LongTensor([example[2] for example in trainset])\n",
    "print(\"pos_ids\")\n",
    "pos_ids = torch.LongTensor([example[3] for example in trainset])\n",
    "print(\"vms\")\n",
    "vms = [example[4] for example in trainset]\n",
    "\n",
    "train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"The number of training instances:\", instances_num)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=args.warmup, t_total=train_steps)\n",
    "\n",
    "total_loss = 0.\n",
    "result = 0.0\n",
    "best_result = 0.0\n",
    "# best_result = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training phase 2\n",
    "for epoch in range(1, args.epochs_num+1):\n",
    "    model.train()\n",
    "    for i, (input_ids_batch, label_ids_batch, seg_ids_batch, pos_ids_batch, vms_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, seg_ids, pos_ids, vms)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        vms_batch = torch.LongTensor(vms_batch)\n",
    "\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        label_ids_batch = label_ids_batch.to(device)\n",
    "        seg_ids_batch = seg_ids_batch.to(device)\n",
    "        pos_ids_batch = pos_ids_batch.to(device)\n",
    "        vms_batch = vms_batch.to(device)\n",
    "\n",
    "        loss, logits = model(\n",
    "            input_ids=input_ids_batch, \n",
    "            attention_mask=vms_batch, \n",
    "            position_ids=pos_ids_batch,\n",
    "            token_type_ids=seg_ids_batch, \n",
    "            label_ids=label_ids_batch\n",
    "         )\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = torch.mean(loss)\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % args.report_steps == 0:\n",
    "            print(\"Epoch id: {}, Training steps: {}, Avg loss: {:.3f}\".format(epoch, i+1, total_loss / args.report_steps))\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Start evaluation on dev dataset.\")\n",
    "    result = evaluate(args, False)\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        save_model(model, args.output_model_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(\"Start evaluation on test dataset.\")\n",
    "    evaluate(args, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation phase.\n",
    "print(\"Final evaluation on the test dataset.\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(torch.load(args.output_model_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.output_model_path))\n",
    "evaluate(args, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
